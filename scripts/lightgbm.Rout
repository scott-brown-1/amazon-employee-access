
R version 4.3.1 (2023-06-16) -- "Beagle Scouts"
Copyright (C) 2023 The R Foundation for Statistical Computing
Platform: x86_64-pc-linux-gnu (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> #########################
> ### Imports and setup ###
> #########################
> 
> library(tidyverse)
── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──
✔ dplyr     1.1.3     ✔ readr     2.1.4
✔ forcats   1.0.0     ✔ stringr   1.5.0
✔ ggplot2   3.4.3     ✔ tibble    3.2.1
✔ lubridate 1.9.3     ✔ tidyr     1.3.0
✔ purrr     1.0.2     
── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
✖ dplyr::filter() masks stats::filter()
✖ dplyr::lag()    masks stats::lag()
ℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors
> library(tidymodels)
── Attaching packages ────────────────────────────────────── tidymodels 1.1.1 ──
✔ broom        1.0.5     ✔ rsample      1.2.0
✔ dials        1.2.0     ✔ tune         1.1.2
✔ infer        1.0.5     ✔ workflows    1.1.3
✔ modeldata    1.2.0     ✔ workflowsets 1.0.1
✔ parsnip      1.1.1     ✔ yardstick    1.2.0
✔ recipes      1.0.8     
── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──
✖ scales::discard() masks purrr::discard()
✖ dplyr::filter()   masks stats::filter()
✖ recipes::fixed()  masks stringr::fixed()
✖ dplyr::lag()      masks stats::lag()
✖ yardstick::spec() masks readr::spec()
✖ recipes::step()   masks stats::step()
• Use tidymodels_prefer() to resolve common conflicts.
> library(doParallel)
Loading required package: foreach

Attaching package: ‘foreach’

The following objects are masked from ‘package:purrr’:

    accumulate, when

Loading required package: iterators
Loading required package: parallel
> library(bonsai)
> 
> #setwd('./amazon-employee-access')
> setwd('..')
> source('./scripts/amazon_analysis.R')
> PARALLEL <- F
> 
> #########################
> ####### Load Data #######
> #########################
> 
> ## Load data
> train <- prep_df(vroom::vroom('./data/train.csv'))
Rows: 32769 Columns: 10
── Column specification ────────────────────────────────────────────────────────
Delimiter: ","
dbl (10): ACTION, RESOURCE, MGR_ID, ROLE_ROLLUP_1, ROLE_ROLLUP_2, ROLE_DEPTN...

ℹ Use `spec()` to retrieve the full column specification for this data.
ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.
> test <- prep_df(vroom::vroom('./data/test.csv'))
Rows: 58921 Columns: 10
── Column specification ────────────────────────────────────────────────────────
Delimiter: ","
dbl (10): id, RESOURCE, MGR_ID, ROLE_ROLLUP_1, ROLE_ROLLUP_2, ROLE_DEPTNAME,...

ℹ Use `spec()` to retrieve the full column specification for this data.
ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.
> 
> #########################
> ## Feature Engineering ##
> #########################
> 
> set.seed(843)
> 
> ## parallel tune grid
> 
> if(PARALLEL){
+   cl <- makePSOCKcluster(5)
+   registerDoParallel(cl)
+ }
> 
> ## Set up preprocessing
> prepped_recipe <- setup_train_recipe(train, encode = F, smote_K = 0, pca_threshold = 0)
> 
> ## Bake recipe
> bake(prepped_recipe, new_data=train)
# A tibble: 32,769 × 10
   RESOURCE MGR_ID ROLE_ROLLUP_1 ROLE_ROLLUP_2 ROLE_DEPTNAME ROLE_TITLE
   <fct>    <fct>  <fct>         <fct>         <fct>         <fct>     
 1 39353    85475  117961        118300        123472        117905    
 2 17183    1540   117961        118343        123125        118536    
 3 36724    14457  118219        118220        117884        117879    
 4 36135    5396   117961        118343        119993        118321    
 5 42680    5905   117929        117930        119569        119323    
 6 45333    14561  117951        117952        118008        118568    
 7 25993    17227  117961        118343        123476        118980    
 8 19666    4209   117961        117969        118910        126820    
 9 31246    783    117961        118413        120584        128230    
10 78766    56683  118079        118080        117878        117879    
# ℹ 32,759 more rows
# ℹ 4 more variables: ROLE_FAMILY_DESC <fct>, ROLE_FAMILY <fct>,
#   ROLE_CODE <fct>, ACTION <fct>
> bake(prepped_recipe, new_data=test)
# A tibble: 58,921 × 9
   RESOURCE MGR_ID ROLE_ROLLUP_1 ROLE_ROLLUP_2 ROLE_DEPTNAME ROLE_TITLE
   <fct>    <fct>  <fct>         <fct>         <fct>         <fct>     
 1 78766    72734  118079        118080        117878        117879    
 2 40644    4378   117961        118327        118507        118863    
 3 75443    2395   117961        118300        119488        118172    
 4 43219    19986  117961        118225        118403        120773    
 5 42093    50015  117961        118343        119598        118422    
 6 44722    1755   117961        117962        119223        125793    
 7 75834    21135  117961        118343        123494        118054    
 8 4675     3077   117961        118300        120312        124194    
 9 18072    15575  117902        118041        118623        280788    
10 22680    4474   117961        118446        119064        118321    
# ℹ 58,911 more rows
# ℹ 3 more variables: ROLE_FAMILY_DESC <fct>, ROLE_FAMILY <fct>,
#   ROLE_CODE <fct>
> 
> #########################
> ## Fit Regression Model #
> #########################
> 
> boost_model <- boost_tree(
+   trees = 200, #tune(), #100
+   tree_depth = 5, #tune(), #1,
+   learn_rate = 0.1,#tune(), #0.1,
+   mtry = 2,#tune(), #3,
+   min_n = 20, #tune(), #20,
+   loss_reduction = 0#tune(), #0
+   ) %>% 
+   set_engine("lightgbm") %>% 
+   set_mode("classification")
> 
> ## Define workflow
> # Transform response to get different cutoff
> boost_wf <- workflow(prepped_recipe) %>%
+   add_model(boost_model)
> 
> ## Grid of values to tune over
> tuning_grid <- grid_regular(
+   trees(),
+   tree_depth(),
+   learn_rate(),
+   mtry(range=c(3,ncol(train))),
+   min_n(),
+   loss_reduction(),
+   levels = 5)
> 
> ## Split data for CV
> folds <- vfold_cv(train, v = 5, repeats=1)
> 
> # Run the CV
> # cv_results <- boost_wf %>%
> #   tune_grid(resamples=folds,
> #             grid=tuning_grid,
> #             metrics=metric_set(roc_auc))
> 
> # Find optimal tuning params
> # best_params <- cv_results %>%
> #   select_best("roc_auc")
> 
> # print(best_params)
> 
> # Fit workflow
> final_wf <- boost_wf %>%
+   #finalize_workflow(best_params) %>%
+   fit(data = train)
> 
> ## Predict new y
> output <- predict(final_wf, new_data=test, type='prob') %>%
+   bind_cols(., test) %>%
+   rename(ACTION=.pred_1) %>%
+   select(id, ACTION)
> 
> #LS: penalty, then mixture
> vroom::vroom_write(output,'./outputs/light_gbm_preds.csv',delim=',')
> 
> if(PARALLEL){
+   stopCluster(cl)
+ }
> 
> proc.time()
    user   system  elapsed 
1763.200    3.356   40.796 
